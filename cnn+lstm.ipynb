{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:44.399630Z",
     "iopub.status.busy": "2024-04-14T04:15:44.399120Z",
     "iopub.status.idle": "2024-04-14T04:15:44.408692Z",
     "shell.execute_reply": "2024-04-14T04:15:44.407643Z",
     "shell.execute_reply.started": "2024-04-14T04:15:44.399594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision.models import densenet201\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import cv2\n",
    "from textwrap import wrap\n",
    "from collections import Counter \n",
    "import pickle\n",
    "import gc\n",
    "import random\n",
    "import spacy \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:44.425764Z",
     "iopub.status.busy": "2024-04-14T04:15:44.425304Z",
     "iopub.status.idle": "2024-04-14T04:15:44.512196Z",
     "shell.execute_reply": "2024-04-14T04:15:44.510928Z",
     "shell.execute_reply.started": "2024-04-14T04:15:44.425730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PATH = '/kaggle/input/flickr8k'\n",
    "data = pd.read_csv(PATH + f'/captions.txt')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:44.515225Z",
     "iopub.status.busy": "2024-04-14T04:15:44.514766Z",
     "iopub.status.idle": "2024-04-14T04:15:44.524859Z",
     "shell.execute_reply": "2024-04-14T04:15:44.523418Z",
     "shell.execute_reply.started": "2024-04-14T04:15:44.515185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img/255\n",
    "    return img\n",
    "\n",
    "def visualize_data(df):\n",
    "    df = df.reset_index(drop= True)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    n = 0 \n",
    "    for i in range(15):\n",
    "        n += 1\n",
    "        plt.subplot(5 , 5, n)\n",
    "        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n",
    "        image = load_image(f\"/kaggle/input/flickr8k/Images/{df.image[i]}\")\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"\\n\".join(wrap(df.caption[i], 20)))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:44.527442Z",
     "iopub.status.busy": "2024-04-14T04:15:44.526701Z",
     "iopub.status.idle": "2024-04-14T04:15:44.553688Z",
     "shell.execute_reply": "2024-04-14T04:15:44.552356Z",
     "shell.execute_reply.started": "2024-04-14T04:15:44.527399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_path = PATH + f'/Images/1000268201_693b08cb0e.jpg'\n",
    "image = load_image(image_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:44.557177Z",
     "iopub.status.busy": "2024-04-14T04:15:44.556699Z",
     "iopub.status.idle": "2024-04-14T04:15:47.481342Z",
     "shell.execute_reply": "2024-04-14T04:15:47.480094Z",
     "shell.execute_reply.started": "2024-04-14T04:15:44.557137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_data(data.sample(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.483462Z",
     "iopub.status.busy": "2024-04-14T04:15:47.483043Z",
     "iopub.status.idle": "2024-04-14T04:15:47.488715Z",
     "shell.execute_reply": "2024-04-14T04:15:47.487515Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.483427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def text_preprocessing(data):\n",
    "#     data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
    "#     data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
    "#     data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
    "#     data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
    "#     data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n",
    "#     return data\n",
    "\n",
    "# data = text_preprocessing(data)\n",
    "# captions = data['caption'].tolist()\n",
    "# captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.490504Z",
     "iopub.status.busy": "2024-04-14T04:15:47.490129Z",
     "iopub.status.idle": "2024-04-14T04:15:47.499273Z",
     "shell.execute_reply": "2024-04-14T04:15:47.497974Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.490466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = get_tokenizer(\"basic_english\")\n",
    "# tokenized_captions = [tokenizer(caption) for caption in captions]\n",
    "\n",
    "# vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_captions)\n",
    "\n",
    "# vocab_size = len(vocab) + 1\n",
    "\n",
    "# max_length = max(len(tokens) for tokens in tokenized_captions)\n",
    "\n",
    "# print(\"Vocabulary size:\", vocab_size)\n",
    "# print(\"Maximum sequence length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.501275Z",
     "iopub.status.busy": "2024-04-14T04:15:47.500903Z",
     "iopub.status.idle": "2024-04-14T04:15:47.507599Z",
     "shell.execute_reply": "2024-04-14T04:15:47.506625Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.501246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# images = data['image'].unique().tolist()\n",
    "# nimages = len(images)\n",
    "\n",
    "# split_index = round(0.85 * nimages)\n",
    "\n",
    "# train_images = images[:split_index]\n",
    "# val_images = images[split_index:]\n",
    "\n",
    "# test = data[data['image'].isin(val_images)]\n",
    "# train = data[data['image'].isin(train_images)]\n",
    "\n",
    "# train.reset_index(inplace=True, drop=True)\n",
    "# test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.508928Z",
     "iopub.status.busy": "2024-04-14T04:15:47.508603Z",
     "iopub.status.idle": "2024-04-14T04:15:47.520777Z",
     "shell.execute_reply": "2024-04-14T04:15:47.519670Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.508900Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def remove_single_char_word(word_list):\n",
    "#     lst = []\n",
    "#     for word in word_list:\n",
    "#         if len(word)>1:\n",
    "#             lst.append(word)\n",
    "\n",
    "#     return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.524855Z",
     "iopub.status.busy": "2024-04-14T04:15:47.524465Z",
     "iopub.status.idle": "2024-04-14T04:15:47.531000Z",
     "shell.execute_reply": "2024-04-14T04:15:47.530032Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.524826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data['cleaned_caption'] = data['caption'].apply(lambda caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in caption.split(\" \")] + ['<end>'])\n",
    "# data['cleaned_caption']  = data['cleaned_caption'].apply(lambda x : remove_single_char_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.532414Z",
     "iopub.status.busy": "2024-04-14T04:15:47.532048Z",
     "iopub.status.idle": "2024-04-14T04:15:47.542015Z",
     "shell.execute_reply": "2024-04-14T04:15:47.541105Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.532383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data['seq_len'] = data['cleaned_caption'].apply(lambda x : len(x))\n",
    "# max_seq_len = data['seq_len'].max()\n",
    "# print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.543429Z",
     "iopub.status.busy": "2024-04-14T04:15:47.543065Z",
     "iopub.status.idle": "2024-04-14T04:15:47.553624Z",
     "shell.execute_reply": "2024-04-14T04:15:47.552556Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.543399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data.drop(['seq_len'], axis = 1, inplace = True)\n",
    "# data['cleaned_caption'] = data['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.555143Z",
     "iopub.status.busy": "2024-04-14T04:15:47.554783Z",
     "iopub.status.idle": "2024-04-14T04:15:47.565278Z",
     "shell.execute_reply": "2024-04-14T04:15:47.563905Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.555113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# display(data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.567600Z",
     "iopub.status.busy": "2024-04-14T04:15:47.566688Z",
     "iopub.status.idle": "2024-04-14T04:15:47.578375Z",
     "shell.execute_reply": "2024-04-14T04:15:47.576916Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.567564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# word_list = data['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\n",
    "# word_dict = Counter(word_list)\n",
    "# word_dict =  sorted(word_dict, key=word_dict.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.580292Z",
     "iopub.status.busy": "2024-04-14T04:15:47.579921Z",
     "iopub.status.idle": "2024-04-14T04:15:47.589375Z",
     "shell.execute_reply": "2024-04-14T04:15:47.588242Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.580263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(len(word_dict))\n",
    "# print(word_dict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.590987Z",
     "iopub.status.busy": "2024-04-14T04:15:47.590451Z",
     "iopub.status.idle": "2024-04-14T04:15:47.601838Z",
     "shell.execute_reply": "2024-04-14T04:15:47.600622Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.590958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# vocab_size = len(word_dict)\n",
    "# print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.603666Z",
     "iopub.status.busy": "2024-04-14T04:15:47.603280Z",
     "iopub.status.idle": "2024-04-14T04:15:47.611991Z",
     "shell.execute_reply": "2024-04-14T04:15:47.610812Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.603638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# index_to_word = {index: word for index, word in enumerate(word_dict)}\n",
    "# word_to_index = {word: index for index, word in enumerate(word_dict)}\n",
    "# print(len(index_to_word), len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.614170Z",
     "iopub.status.busy": "2024-04-14T04:15:47.613803Z",
     "iopub.status.idle": "2024-04-14T04:15:47.623871Z",
     "shell.execute_reply": "2024-04-14T04:15:47.622636Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.614139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data['text_seq']  = data['cleaned_caption'].apply(lambda caption : [word_to_index[word] for word in caption] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.626508Z",
     "iopub.status.busy": "2024-04-14T04:15:47.626015Z",
     "iopub.status.idle": "2024-04-14T04:15:47.635349Z",
     "shell.execute_reply": "2024-04-14T04:15:47.634396Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.626472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# display(data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.637076Z",
     "iopub.status.busy": "2024-04-14T04:15:47.636704Z",
     "iopub.status.idle": "2024-04-14T04:15:47.646415Z",
     "shell.execute_reply": "2024-04-14T04:15:47.645087Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.637036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data = data.sort_values(by = 'image')\n",
    "# train = data.iloc[:int(0.9*len(data))]\n",
    "# valid = data.iloc[int(0.9*len(data)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:47.649582Z",
     "iopub.status.busy": "2024-04-14T04:15:47.649181Z",
     "iopub.status.idle": "2024-04-14T04:15:48.623006Z",
     "shell.execute_reply": "2024-04-14T04:15:48.621813Z",
     "shell.execute_reply.started": "2024-04-14T04:15:47.649550Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#using spacy for the better text tokenization \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#example\n",
    "text = \"This is a good place to find a city\"\n",
    "[token.text.lower() for token in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.624721Z",
     "iopub.status.busy": "2024-04-14T04:15:48.624387Z",
     "iopub.status.idle": "2024-04-14T04:15:48.635502Z",
     "shell.execute_reply": "2024-04-14T04:15:48.634134Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.624695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        #setting the pre-reserved tokens int to string tokens\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        \n",
    "        #string to int tokens\n",
    "        #its reverse dict self.itos\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in nlp.tokenizer(text)]\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                #add the word to the vocab if it reaches minum frequecy threshold\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self,text):\n",
    "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.638240Z",
     "iopub.status.busy": "2024-04-14T04:15:48.637015Z",
     "iopub.status.idle": "2024-04-14T04:15:48.647694Z",
     "shell.execute_reply": "2024-04-14T04:15:48.646252Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.638205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#testing the vicab class \n",
    "v = Vocabulary(freq_threshold=1)\n",
    "\n",
    "v.build_vocab([\"This is a good place to find a city\"])\n",
    "print(v.stoi)\n",
    "print(v.numericalize(\"This is a good place to find a city here!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.654676Z",
     "iopub.status.busy": "2024-04-14T04:15:48.654174Z",
     "iopub.status.idle": "2024-04-14T04:15:48.660565Z",
     "shell.execute_reply": "2024-04-14T04:15:48.659388Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.654641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_samples = len(train)\n",
    "# print(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.663662Z",
     "iopub.status.busy": "2024-04-14T04:15:48.663032Z",
     "iopub.status.idle": "2024-04-14T04:15:48.671750Z",
     "shell.execute_reply": "2024-04-14T04:15:48.670601Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.663615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# unq_train_imgs = train[['image']].drop_duplicates()\n",
    "# unq_valid_imgs = valid[['image']].drop_duplicates()\n",
    "# print(len(unq_train_imgs), len(unq_valid_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.674695Z",
     "iopub.status.busy": "2024-04-14T04:15:48.674079Z",
     "iopub.status.idle": "2024-04-14T04:15:48.683711Z",
     "shell.execute_reply": "2024-04-14T04:15:48.682147Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.674648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.686560Z",
     "iopub.status.busy": "2024-04-14T04:15:48.685997Z",
     "iopub.status.idle": "2024-04-14T04:15:48.699861Z",
     "shell.execute_reply": "2024-04-14T04:15:48.698517Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.686491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class extractImageFeatureResNetDataSet():\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data \n",
    "#         self.scaler = transforms.Resize([224, 224])\n",
    "#         self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225])\n",
    "#         self.to_tensor = transforms.ToTensor()\n",
    "#     def __len__(self):  \n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "\n",
    "#         image_name = self.data.iloc[idx]['image']\n",
    "#         img_loc = '../input/flickr8k/Images/'+str(image_name)\n",
    "\n",
    "#         img = Image.open(img_loc)\n",
    "#         t_img = self.normalize(self.to_tensor(self.scaler(img)))\n",
    "\n",
    "#         return image_name, t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.701811Z",
     "iopub.status.busy": "2024-04-14T04:15:48.701409Z",
     "iopub.status.idle": "2024-04-14T04:15:48.710458Z",
     "shell.execute_reply": "2024-04-14T04:15:48.709094Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.701779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_train_imgs)\n",
    "# train_ImageDataloader_ResNet = DataLoader(train_ImageDataset_ResNet, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.712189Z",
     "iopub.status.busy": "2024-04-14T04:15:48.711853Z",
     "iopub.status.idle": "2024-04-14T04:15:48.722119Z",
     "shell.execute_reply": "2024-04-14T04:15:48.721216Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.712162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# valid_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_valid_imgs)\n",
    "# valid_ImageDataloader_ResNet = DataLoader(valid_ImageDataset_ResNet, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.723659Z",
     "iopub.status.busy": "2024-04-14T04:15:48.723349Z",
     "iopub.status.idle": "2024-04-14T04:15:48.734269Z",
     "shell.execute_reply": "2024-04-14T04:15:48.733003Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.723633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# resnet18 = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "# resnet18.eval()\n",
    "# list(resnet18._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.736342Z",
     "iopub.status.busy": "2024-04-14T04:15:48.735955Z",
     "iopub.status.idle": "2024-04-14T04:15:48.745867Z",
     "shell.execute_reply": "2024-04-14T04:15:48.744462Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.736293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# resNet18Layer4 = resnet18._modules.get('layer4').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.748051Z",
     "iopub.status.busy": "2024-04-14T04:15:48.747665Z",
     "iopub.status.idle": "2024-04-14T04:15:48.755663Z",
     "shell.execute_reply": "2024-04-14T04:15:48.754690Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.748023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_vector(t_img):\n",
    "    \n",
    "#     t_img = Variable(t_img)\n",
    "#     my_embedding = torch.zeros(1, 512, 7, 7)\n",
    "#     def copy_data(m, i, o):\n",
    "#         my_embedding.copy_(o.data)\n",
    "    \n",
    "#     h = resNet18Layer4.register_forward_hook(copy_data)\n",
    "#     resnet18(t_img)\n",
    "    \n",
    "#     h.remove()\n",
    "#     return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:48.757736Z",
     "iopub.status.busy": "2024-04-14T04:15:48.756907Z",
     "iopub.status.idle": "2024-04-14T04:15:48.765146Z",
     "shell.execute_reply": "2024-04-14T04:15:48.764378Z",
     "shell.execute_reply.started": "2024-04-14T04:15:48.757705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# extract_imgFtr_ResNet_train = {}\n",
    "# for image_name, t_img in tqdm(train_ImageDataloader_ResNet):\n",
    "#     t_img = t_img.to(device)\n",
    "#     embdg = get_vector(t_img)\n",
    "    \n",
    "#     extract_imgFtr_ResNet_train[image_name[0]] = embdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.210771Z",
     "iopub.status.busy": "2024-04-14T04:15:49.210246Z",
     "iopub.status.idle": "2024-04-14T04:15:49.216163Z",
     "shell.execute_reply": "2024-04-14T04:15:49.214636Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.210732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# a_file = open(\"./EncodedImageTrainResNet.pkl\", \"wb\")\n",
    "# pickle.dump(extract_imgFtr_ResNet_train, a_file)\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.218338Z",
     "iopub.status.busy": "2024-04-14T04:15:49.217900Z",
     "iopub.status.idle": "2024-04-14T04:15:49.226976Z",
     "shell.execute_reply": "2024-04-14T04:15:49.225740Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.218277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# extract_imgFtr_ResNet_valid = {}\n",
    "# for image_name, t_img in tqdm(valid_ImageDataloader_ResNet):\n",
    "#     t_img = t_img.to(device)\n",
    "#     embdg = get_vector(t_img)\n",
    " \n",
    "#     extract_imgFtr_ResNet_valid[image_name[0]] = embdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.228739Z",
     "iopub.status.busy": "2024-04-14T04:15:49.228390Z",
     "iopub.status.idle": "2024-04-14T04:15:49.241133Z",
     "shell.execute_reply": "2024-04-14T04:15:49.240025Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.228710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# a_file = open(\"./EncodedImageValidResNet.pkl\", \"wb\")\n",
    "# pickle.dump(extract_imgFtr_ResNet_valid, a_file)\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.242813Z",
     "iopub.status.busy": "2024-04-14T04:15:49.242432Z",
     "iopub.status.idle": "2024-04-14T04:15:49.252428Z",
     "shell.execute_reply": "2024-04-14T04:15:49.251177Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.242782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.models import densenet201\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = densenet201(pretrained=True)\n",
    "# fe = torch.nn.Sequential(*list(model.features.children())[:-1])\n",
    "# fe.to(device)\n",
    "# fe.eval()\n",
    "\n",
    "# img_size = 224\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((img_size, img_size)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# features = {}\n",
    "# image_path = \"/kaggle/input/flickr8k/Images\"  \n",
    "# for image in tqdm(data['image'].unique().tolist()):\n",
    "#     img = Image.open(os.path.join(image_path, image))\n",
    "#     img = transform(img)\n",
    "#     img = img.unsqueeze(0).to(device)\n",
    "#     feature = fe(img).detach().cpu().numpy()\n",
    "#     features[image] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.256541Z",
     "iopub.status.busy": "2024-04-14T04:15:49.256102Z",
     "iopub.status.idle": "2024-04-14T04:15:49.263788Z",
     "shell.execute_reply": "2024-04-14T04:15:49.262841Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.256504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, df, X_col, y_col, directory, tokenizer, vocab_size, max_length, features):\n",
    "#         self.df = df.copy()\n",
    "#         self.X_col = X_col\n",
    "#         self.y_col = y_col\n",
    "#         self.directory = directory\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.max_length = max_length\n",
    "#         self.features = features\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         image_path = self.df.iloc[index][self.X_col]\n",
    "#         image_feature = torch.tensor(self.features[image_path][0], dtype=torch.float32)\n",
    "        \n",
    "#         caption = self.df.iloc[index][self.y_col]\n",
    "#         caption_sequence = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "#         caption_input = []\n",
    "#         target = []\n",
    "#         for i in range(1, len(caption_sequence)):\n",
    "#             in_seq, out_seq = caption_sequence[:i], caption_sequence[i]\n",
    "#             in_seq = torch.tensor(pad_sequences([in_seq], maxlen=self.max_length)[0], dtype=torch.long)\n",
    "#             out_seq = torch.tensor(to_categorical([out_seq], num_classes=self.vocab_size)[0], dtype=torch.float32)\n",
    "#             caption_input.append(in_seq)\n",
    "#             target.append(out_seq)\n",
    "        \n",
    "#         caption_input = torch.stack(caption_input)\n",
    "#         target = torch.stack(target)\n",
    "        \n",
    "#         return image_feature, caption_input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.266693Z",
     "iopub.status.busy": "2024-04-14T04:15:49.265577Z",
     "iopub.status.idle": "2024-04-14T04:15:49.276790Z",
     "shell.execute_reply": "2024-04-14T04:15:49.275594Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.266647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_dataset = CustomDataset(df=train,X_col='image',y_col='caption',directory=image_path,\n",
    "#                                       tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n",
    "# test_dataset = CustomDataset(df=test,X_col='image',y_col='caption',directory=image_path,\n",
    "#                                       tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.280790Z",
     "iopub.status.busy": "2024-04-14T04:15:49.278165Z",
     "iopub.status.idle": "2024-04-14T04:15:49.287613Z",
     "shell.execute_reply": "2024-04-14T04:15:49.286461Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.280753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class FlickerDataSetResnet():\n",
    "#     def __init__(self, data, pkl_file):\n",
    "#         self.data = data\n",
    "#         self.encodedImgs = pd.read_pickle(pkl_file)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "    \n",
    "#         caption_seq = self.data.iloc[idx]['text_seq']\n",
    "#         target_seq = caption_seq[1:]+[0]\n",
    "\n",
    "#         image_name = self.data.iloc[idx]['image']\n",
    "#         image_tensor = self.encodedImgs[image_name]\n",
    "#         image_tensor = image_tensor.permute(0,2,3,1)\n",
    "#         image_tensor_view = image_tensor.view(image_tensor.size(0), -1, image_tensor.size(3))\n",
    "\n",
    "#         return torch.tensor(caption_seq), torch.tensor(target_seq), image_tensor_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.290005Z",
     "iopub.status.busy": "2024-04-14T04:15:49.289140Z",
     "iopub.status.idle": "2024-04-14T04:15:49.302758Z",
     "shell.execute_reply": "2024-04-14T04:15:49.301710Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.289962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_dataset_resnet = FlickerDataSetResnet(train, 'EncodedImageTrainResNet.pkl')\n",
    "# train_dataloader_resnet = DataLoader(train_dataset_resnet, batch_size=32, shuffle=True)\n",
    "\n",
    "# valid_dataset_resnet = FlickerDataSetResnet(valid, 'EncodedImageValidResNet.pkl')\n",
    "# valid_dataloader_resnet = DataLoader(valid_dataset_resnet, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.304779Z",
     "iopub.status.busy": "2024-04-14T04:15:49.304379Z",
     "iopub.status.idle": "2024-04-14T04:15:49.314998Z",
     "shell.execute_reply": "2024-04-14T04:15:49.314004Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.304745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FlickrDataset\n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir,caption_file,transform=None,freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(caption_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "        #Get image and caption colum from the dataframe\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "        \n",
    "        #Initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions.tolist())\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.captions[idx]\n",
    "        img_name = self.imgs[idx]\n",
    "        img_location = os.path.join(self.root_dir,img_name)\n",
    "        img = Image.open(img_location).convert(\"RGB\")\n",
    "        \n",
    "        #apply the transfromation to the image\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        #numericalize the caption text\n",
    "        caption_vec = []\n",
    "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
    "        caption_vec += self.vocab.numericalize(caption)\n",
    "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
    "        \n",
    "        return img, torch.tensor(caption_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:49.317103Z",
     "iopub.status.busy": "2024-04-14T04:15:49.316562Z",
     "iopub.status.idle": "2024-04-14T04:15:51.325476Z",
     "shell.execute_reply": "2024-04-14T04:15:51.324187Z",
     "shell.execute_reply.started": "2024-04-14T04:15:49.317071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Initiate the Dataset and Dataloader\n",
    "\n",
    "#setting the constants\n",
    "data_location =  \"../input/flickr8k\"\n",
    "BATCH_SIZE = 256\n",
    "# BATCH_SIZE = 6\n",
    "NUM_WORKER = 4\n",
    "\n",
    "#defining the transform to be applied\n",
    "transforms = T.Compose([\n",
    "    T.Resize(226),                     \n",
    "    T.RandomCrop(224),                 \n",
    "    T.ToTensor(),                               \n",
    "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "#testing the dataset class\n",
    "dataset =  FlickrDataset(\n",
    "    root_dir = '/kaggle/input/flickr8k/Images',\n",
    "    caption_file = '/kaggle/input/flickr8k/captions.txt',\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "#writing the dataloader\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "#vocab_size\n",
    "vocab_size = len(dataset.vocab)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.327790Z",
     "iopub.status.busy": "2024-04-14T04:15:51.327433Z",
     "iopub.status.idle": "2024-04-14T04:15:51.332538Z",
     "shell.execute_reply": "2024-04-14T04:15:51.331399Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.327760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class EncoderCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(EncoderCNN, self).__init__()\n",
    "#         resnet = models.resnet50(pretrained=True)\n",
    "#         for param in resnet.parameters():\n",
    "#             param.requires_grad_(False)\n",
    "        \n",
    "#         modules = list(resnet.children())[:-2]\n",
    "#         self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "#     def forward(self, images):\n",
    "#         features = self.resnet(images)                                    #(batch_size,2048,7,7)\n",
    "#         features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n",
    "#         features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n",
    "#         return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.334526Z",
     "iopub.status.busy": "2024-04-14T04:15:51.334142Z",
     "iopub.status.idle": "2024-04-14T04:15:51.345614Z",
     "shell.execute_reply": "2024-04-14T04:15:51.344438Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.334488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #Bahdanau Attention\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
    "#         super(Attention, self).__init__()\n",
    "        \n",
    "#         self.attention_dim = attention_dim\n",
    "        \n",
    "#         self.W = nn.Linear(decoder_dim,attention_dim)\n",
    "#         self.U = nn.Linear(encoder_dim,attention_dim)\n",
    "        \n",
    "#         self.A = nn.Linear(attention_dim,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#     def forward(self, features, hidden_state):\n",
    "#         u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n",
    "#         w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
    "        \n",
    "#         combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n",
    "        \n",
    "#         attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n",
    "#         attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n",
    "        \n",
    "        \n",
    "#         alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n",
    "        \n",
    "#         attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n",
    "#         attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n",
    "        \n",
    "#         return alpha,attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.347818Z",
     "iopub.status.busy": "2024-04-14T04:15:51.347356Z",
     "iopub.status.idle": "2024-04-14T04:15:51.361434Z",
     "shell.execute_reply": "2024-04-14T04:15:51.360070Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.347775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #Attention Decoder\n",
    "# class DecoderRNN(nn.Module):\n",
    "#     def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         #save the model param\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.attention_dim = attention_dim\n",
    "#         self.decoder_dim = decoder_dim\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "#         self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n",
    "        \n",
    "        \n",
    "#         self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "#         self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "#         self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n",
    "#         self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        \n",
    "        \n",
    "#         self.fcn = nn.Linear(decoder_dim,vocab_size)\n",
    "#         self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "    \n",
    "#     def forward(self, features, captions):\n",
    "        \n",
    "#         #vectorize the caption\n",
    "#         embeds = self.embedding(captions)\n",
    "        \n",
    "#         # Initialize LSTM state\n",
    "#         h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "#         #get the seq length to iterate\n",
    "#         seq_length = len(captions[0])-1 #Exclude the last one\n",
    "#         batch_size = captions.size(0)\n",
    "#         num_features = features.size(1)\n",
    "        \n",
    "#         preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
    "#         alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
    "                \n",
    "#         for s in range(seq_length):\n",
    "#             alpha,context = self.attention(features, h)\n",
    "#             lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
    "#             h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                    \n",
    "#             output = self.fcn(self.drop(h))\n",
    "            \n",
    "#             preds[:,s] = output\n",
    "#             alphas[:,s] = alpha  \n",
    "        \n",
    "        \n",
    "#         return preds, alphas\n",
    "    \n",
    "#     def generate_caption(self,features,max_len=20,vocab=None):\n",
    "#         # Inference part\n",
    "#         # Given the image features generate the captions\n",
    "        \n",
    "#         batch_size = features.size(0)\n",
    "#         h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "#         alphas = []\n",
    "        \n",
    "#         #starting input\n",
    "#         word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
    "#         embeds = self.embedding(word)\n",
    "\n",
    "        \n",
    "#         captions = []\n",
    "        \n",
    "#         for i in range(max_len):\n",
    "#             alpha,context = self.attention(features, h)\n",
    "            \n",
    "            \n",
    "#             #store the apla score\n",
    "#             alphas.append(alpha.cpu().detach().numpy())\n",
    "            \n",
    "#             lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
    "#             h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "#             output = self.fcn(self.drop(h))\n",
    "#             output = output.view(batch_size,-1)\n",
    "        \n",
    "            \n",
    "#             #select the word with most val\n",
    "#             predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "#             #save the generated word\n",
    "#             captions.append(predicted_word_idx.item())\n",
    "            \n",
    "#             #end if <EOS detected>\n",
    "#             if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "#                 break\n",
    "            \n",
    "#             #send generated word as the next caption\n",
    "#             embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "#         #covert the vocab idx to words and return sentence\n",
    "#         return [vocab.itos[idx] for idx in captions],alphas\n",
    "    \n",
    "    \n",
    "#     def init_hidden_state(self, encoder_out):\n",
    "#         mean_encoder_out = encoder_out.mean(dim=1)\n",
    "#         h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "#         c = self.init_c(mean_encoder_out)\n",
    "#         return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.364134Z",
     "iopub.status.busy": "2024-04-14T04:15:51.363374Z",
     "iopub.status.idle": "2024-04-14T04:15:51.374946Z",
     "shell.execute_reply": "2024-04-14T04:15:51.373599Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.364090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class EncoderDecoder(nn.Module):\n",
    "#     def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "#         super().__init__()\n",
    "#         self.encoder = EncoderCNN()\n",
    "#         self.decoder = DecoderRNN(\n",
    "#             embed_size=embed_size,\n",
    "#             vocab_size = vocab_size,\n",
    "#             attention_dim=attention_dim,\n",
    "#             encoder_dim=encoder_dim,\n",
    "#             decoder_dim=decoder_dim\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, images, captions):\n",
    "#         features = self.encoder(images)\n",
    "#         outputs = self.decoder(features, captions)\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.377036Z",
     "iopub.status.busy": "2024-04-14T04:15:51.376695Z",
     "iopub.status.idle": "2024-04-14T04:15:51.389045Z",
     "shell.execute_reply": "2024-04-14T04:15:51.387928Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.377008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# embed_size=300\n",
    "# vocab_size = vocab_size\n",
    "# attention_dim=256\n",
    "# encoder_dim=2048\n",
    "# decoder_dim=512\n",
    "# learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.391168Z",
     "iopub.status.busy": "2024-04-14T04:15:51.390736Z",
     "iopub.status.idle": "2024-04-14T04:15:51.399119Z",
     "shell.execute_reply": "2024-04-14T04:15:51.398046Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.391128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = EncoderDecoder(\n",
    "#     embed_size=300,\n",
    "#     vocab_size = vocab_size,\n",
    "#     attention_dim=256,\n",
    "#     encoder_dim=2048,\n",
    "#     decoder_dim=512\n",
    "# ).to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.402462Z",
     "iopub.status.busy": "2024-04-14T04:15:51.402097Z",
     "iopub.status.idle": "2024-04-14T04:15:51.409764Z",
     "shell.execute_reply": "2024-04-14T04:15:51.408923Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.402431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def save_model(model,num_epochs):\n",
    "#     model_state = {\n",
    "#         'num_epochs':num_epochs,\n",
    "#         'embed_size':embed_size,\n",
    "#         'vocab_size':len(dataset.vocab),\n",
    "#         'attention_dim':attention_dim,\n",
    "#         'encoder_dim':encoder_dim,\n",
    "#         'decoder_dim':decoder_dim,\n",
    "#         'state_dict':model.state_dict()\n",
    "#     }\n",
    "\n",
    "#     torch.save(model_state,'attention_model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.412093Z",
     "iopub.status.busy": "2024-04-14T04:15:51.411429Z",
     "iopub.status.idle": "2024-04-14T04:15:51.419791Z",
     "shell.execute_reply": "2024-04-14T04:15:51.418806Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.412053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# num_epochs = 25\n",
    "# print_every = 100\n",
    "\n",
    "# for epoch in range(1,num_epochs+1):   \n",
    "#     for idx, (image, captions) in enumerate(iter(train_dataloader)):\n",
    "#         image,captions = image.to(device),captions.to(device)\n",
    "\n",
    "#         # Zero the gradients.\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Feed forward\n",
    "#         outputs,attentions = model(image, captions)\n",
    "\n",
    "#         # Calculate the batch loss.\n",
    "#         targets = captions[:,1:]\n",
    "#         loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "        \n",
    "#         # Backward pass.\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Update the parameters in the optimizer.\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (idx+1)%print_every == 0:\n",
    "#             print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "            \n",
    "            \n",
    "#             #generate the caption\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 dataiter = iter(train_dataloader)\n",
    "#                 img,_ = next(dataiter)\n",
    "#                 features = model.encoder(img[0:1].to(device))\n",
    "#                 caps,alphas = model.decoder.generate_caption(features,vocab=vocab)\n",
    "#                 caption = ' '.join(caps)\n",
    "#                 show_image(img[0],title=caption)\n",
    "                \n",
    "#             model.train()\n",
    "        \n",
    "#     #save the latest model\n",
    "#     save_model(model,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.422101Z",
     "iopub.status.busy": "2024-04-14T04:15:51.421414Z",
     "iopub.status.idle": "2024-04-14T04:15:51.433829Z",
     "shell.execute_reply": "2024-04-14T04:15:51.432854Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.422060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.436175Z",
     "iopub.status.busy": "2024-04-14T04:15:51.435494Z",
     "iopub.status.idle": "2024-04-14T04:15:51.445598Z",
     "shell.execute_reply": "2024-04-14T04:15:51.444534Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.436136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n",
    "        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n",
    "        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.448157Z",
     "iopub.status.busy": "2024-04-14T04:15:51.447404Z",
     "iopub.status.idle": "2024-04-14T04:15:51.461985Z",
     "shell.execute_reply": "2024-04-14T04:15:51.460895Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.448113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Bahdanau Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim,attention_dim)\n",
    "        \n",
    "        self.A = nn.Linear(attention_dim,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, features, hidden_state):\n",
    "        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n",
    "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
    "        \n",
    "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n",
    "        \n",
    "        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n",
    "        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n",
    "        \n",
    "        \n",
    "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n",
    "        \n",
    "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n",
    "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n",
    "        \n",
    "        return alpha,attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.463865Z",
     "iopub.status.busy": "2024-04-14T04:15:51.463443Z",
     "iopub.status.idle": "2024-04-14T04:15:51.483488Z",
     "shell.execute_reply": "2024-04-14T04:15:51.482186Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.463828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Attention Decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #save the model param\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n",
    "        \n",
    "        \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        \n",
    "        \n",
    "        self.fcn = nn.Linear(decoder_dim,vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        #vectorize the caption\n",
    "        embeds = self.embedding(captions)\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        #get the seq length to iterate\n",
    "        seq_length = len(captions[0])-1 #Exclude the last one\n",
    "        batch_size = captions.size(0)\n",
    "        num_features = features.size(1)\n",
    "        \n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
    "                \n",
    "        for s in range(seq_length):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                    \n",
    "            output = self.fcn(self.drop(h))\n",
    "            \n",
    "            preds[:,s] = output\n",
    "            alphas[:,s] = alpha  \n",
    "        \n",
    "        \n",
    "        return preds, alphas\n",
    "    \n",
    "    def generate_caption(self,features,max_len=20,vocab=None):\n",
    "        # Inference part\n",
    "        # Given the image features generate the captions\n",
    "        \n",
    "        batch_size = features.size(0)\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        alphas = []\n",
    "        \n",
    "        #starting input\n",
    "        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
    "        embeds = self.embedding(word)\n",
    "\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            \n",
    "            \n",
    "            #store the apla score\n",
    "            alphas.append(alpha.cpu().detach().numpy())\n",
    "            \n",
    "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(self.drop(h))\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            \n",
    "            #select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            #save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            #end if <EOS detected>\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            #send generated word as the next caption\n",
    "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [vocab.itos[idx] for idx in captions],alphas\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.485747Z",
     "iopub.status.busy": "2024-04-14T04:15:51.485107Z",
     "iopub.status.idle": "2024-04-14T04:15:51.497668Z",
     "shell.execute_reply": "2024-04-14T04:15:51.496407Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.485714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderRNN(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size = len(dataset.vocab),\n",
    "            attention_dim=attention_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.499684Z",
     "iopub.status.busy": "2024-04-14T04:15:51.499260Z",
     "iopub.status.idle": "2024-04-14T04:15:51.512361Z",
     "shell.execute_reply": "2024-04-14T04:15:51.511227Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.499647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embed_size=300\n",
    "vocab_size = len(dataset.vocab)\n",
    "attention_dim=256\n",
    "encoder_dim=2048\n",
    "decoder_dim=512\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:51.516103Z",
     "iopub.status.busy": "2024-04-14T04:15:51.513837Z",
     "iopub.status.idle": "2024-04-14T04:15:52.245417Z",
     "shell.execute_reply": "2024-04-14T04:15:52.244085Z",
     "shell.execute_reply.started": "2024-04-14T04:15:51.516068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = EncoderDecoder(\n",
    "    embed_size=300,\n",
    "    vocab_size = len(dataset.vocab),\n",
    "    attention_dim=256,\n",
    "    encoder_dim=2048,\n",
    "    decoder_dim=512\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:52.247525Z",
     "iopub.status.busy": "2024-04-14T04:15:52.247150Z",
     "iopub.status.idle": "2024-04-14T04:15:52.254334Z",
     "shell.execute_reply": "2024-04-14T04:15:52.253142Z",
     "shell.execute_reply.started": "2024-04-14T04:15:52.247494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_model(model,num_epochs):\n",
    "    model_state = {\n",
    "        'num_epochs':num_epochs,\n",
    "        'embed_size':embed_size,\n",
    "        'vocab_size':len(dataset.vocab),\n",
    "        'attention_dim':attention_dim,\n",
    "        'encoder_dim':encoder_dim,\n",
    "        'decoder_dim':decoder_dim,\n",
    "        'state_dict':model.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(model_state,'attention_model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T04:15:52.255877Z",
     "iopub.status.busy": "2024-04-14T04:15:52.255564Z",
     "iopub.status.idle": "2024-04-14T04:15:58.099003Z",
     "shell.execute_reply": "2024-04-14T04:15:58.092887Z",
     "shell.execute_reply.started": "2024-04-14T04:15:52.255851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(1,num_epochs+1):   \n",
    "    for idx, (image, captions) in enumerate(iter(data_loader)):\n",
    "        image,captions = image.to(device),captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        outputs,attentions = model(image, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        targets = captions[:,1:]\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx+1)%print_every == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "            \n",
    "            \n",
    "            #generate the caption\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dataiter = iter(data_loader)\n",
    "                img,_ = next(dataiter)\n",
    "                features = model.encoder(img[0:1].to(device))\n",
    "                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
    "                caption = ' '.join(caps)\n",
    "                show_image(img[0],title=caption)\n",
    "                \n",
    "            model.train()\n",
    "        \n",
    "    #save the latest model\n",
    "    save_model(model,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
